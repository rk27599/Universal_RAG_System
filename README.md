# Secure RAG System - Web Application

A production-ready **Retrieval-Augmented Generation (RAG) web application** with complete data sovereignty. Features secure authentication, document upload, semantic search, real-time chat with local LLMs, and PostgreSQL vector storage.

## ğŸš€ Key Features

### Core Features
- **ğŸ” Secure Authentication**: JWT-based auth with bcrypt password hashing
- **ğŸ“„ Advanced Document Processing**: Upload HTML, PDF, TXT files with async processing + real-time progress tracking
- **ğŸ§  BGE-M3 Embeddings**: State-of-the-art 1024-dim embeddings (16x larger context window than MiniLM)
- **ğŸ’¬ Real-Time Chat**: WebSocket-based chat with streaming responses and Redis session management
- **ğŸ¤– Local LLM Integration**: Dual provider support (Ollama + vLLM) for Mistral, Llama2, CodeLlama with configurable system prompts
- **ğŸ“Š Rich Metadata**: Track sources, sections, and similarity scores
- **ğŸ¨ Modern UI**: React + TypeScript + Material-UI frontend
- **âš¡ High Performance**: Async document processing, connection pooling, memory optimization
- **ğŸ›¡ï¸ Security-First**: Local-only deployment, no external dependencies
- **ğŸ“ˆ Production-Ready**: Multi-worker support, SSL/TLS, rate limiting, Redis WebSocket management

### ğŸ†• Enhanced RAG Features (October 2024)
- **ğŸ¯ Reranker Service**: Cross-encoder model for re-ranking search results (higher precision)
- **ğŸ” Hybrid Search**: BM25 (keyword) + Vector (semantic) search with ensemble retrieval
- **ğŸ§© Query Expansion**: Multi-query generation for comprehensive retrieval coverage
- **âœ… Corrective RAG**: Self-grading retrieval with web search fallback for missing knowledge
- **ğŸ“ System Prompt Configuration**: Customizable expert prompts with citation enforcement
- **ğŸ§  Memory Manager**: Adaptive batching and model unloading for OOM prevention
- **ğŸ“Š Document Progress Tracking**: Real-time WebSocket updates + API polling during processing

## ğŸ“‹ Requirements

- **Python 3.12+** (migrated from 3.10 - October 2024, ~10-15% faster)
- **PyTorch 2.6.0+** with CUDA 12.4+ (required for BGE-M3 embeddings)
- **Node.js 18+** (for frontend)
- **PostgreSQL 14+** with pgvector extension
- **Redis 7.0+** (for WebSocket session management)
- **Ollama** (for LLM integration)
- **GPU Recommended**: 8GB+ VRAM for optimal embedding/reranker performance

## ğŸ”§ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/rk27599/Python_RAG.git
cd Python_RAG
```

### 2. Setup Backend
```bash
cd webapp/backend
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your database credentials

# Initialize database
python init_db.py
```

### 3. Setup Frontend
```bash
cd webapp/frontend
npm install
npm start  # Development server on http://localhost:3000
```

### 4. Start LLM Provider

**Option A: Ollama (Default - Recommended for Development)**
```bash
# Install from https://ollama.ai
ollama serve
ollama pull mistral
```

**Option B: vLLM (âœ… Production Ready - Multi-User)**
```bash
# Quick start with provided script
bash webapp/scripts/start_vllm.sh

# The script will guide you through setup with:
# - Qwen3-4B-Thinking-2507-FP8 (optimized for 8GB GPU)
# - 16K context window
# - Docker-based deployment (WSL2 compatible)
# - Built-in health checks

# See webapp/docs/VLLM_GUIDE.md for detailed setup
```

### 5. Run Application
```bash
# Backend (from webapp/backend/)
python main.py

# Frontend (from webapp/frontend/)
npm start
```

Access the application at **http://localhost:3000**

## ğŸ“ Project Structure

```
/home/rkpatel/RAG/
â”œâ”€â”€ README.md                        # This file - Project overview
â”œâ”€â”€ CLAUDE.md                        # Project instructions for AI assistants
â”œâ”€â”€ LICENSE                          # Open source license
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”‚
â”œâ”€â”€ logs/                            # Application logs (gitignored)
â”œâ”€â”€ data/                            # Data files (gitignored)
â”œâ”€â”€ archive/                         # Old RAG system (reference)
â”œâ”€â”€ venv/                            # Python virtual environment
â”‚
â””â”€â”€ webapp/                          # ğŸ¯ Main Web Application (ALL code & docs here)
    â”œâ”€â”€ requirements.txt             # Dev/testing dependencies
    â”œâ”€â”€ tests/                       # All test files
    â”‚   â”œâ”€â”€ test_enhanced_rag_e2e.py     # End-to-end RAG tests
    â”‚   â”œâ”€â”€ test_pdf_sample.py           # PDF processing tests
    â”‚   â”œâ”€â”€ test_phase3_integration.py   # Phase 3 integration tests
    â”‚   â””â”€â”€ validate_phases_1_2.py       # Phase 1 & 2 validation
    â”‚
    â”œâ”€â”€ README.md                    # Webapp overview
    â”œâ”€â”€ INSTALLATION_GUIDE.md        # Detailed setup instructions
    â”‚
    â”œâ”€â”€ backend/                     # FastAPI Backend (Python)
    â”‚   â”œâ”€â”€ api/                   # REST API endpoints
    â”‚   â”‚   â”œâ”€â”€ auth.py           # Authentication
    â”‚   â”‚   â”œâ”€â”€ chat.py           # WebSocket chat + messages + Redis
    â”‚   â”‚   â”œâ”€â”€ documents.py      # Document upload/management
    â”‚   â”‚   â””â”€â”€ models.py         # Ollama model info
    â”‚   â”‚
    â”‚   â”œâ”€â”€ core/                  # Core configuration
    â”‚   â”‚   â”œâ”€â”€ config.py         # Settings management + Redis config
    â”‚   â”‚   â”œâ”€â”€ database.py       # SQLAlchemy setup
    â”‚   â”‚   â””â”€â”€ security.py       # JWT & password hashing
    â”‚   â”‚
    â”‚   â”œâ”€â”€ models/                # Database models
    â”‚   â”‚   â”œâ”€â”€ user.py           # User model
    â”‚   â”‚   â”œâ”€â”€ document.py       # Document & Chunk models (progress tracking)
    â”‚   â”‚   â””â”€â”€ conversation.py   # Chat models
    â”‚   â”‚
    â”‚   â”œâ”€â”€ services/              # Business logic
    â”‚   â”‚   â”œâ”€â”€ document_service.py         # Document processing with progress
    â”‚   â”‚   â”œâ”€â”€ embedding_service_bge.py    # BGE-M3 embeddings (1024-dim)
    â”‚   â”‚   â”œâ”€â”€ pdf_processor.py            # Advanced PDF processing
    â”‚   â”‚   â”œâ”€â”€ enhanced_search_service.py  # ğŸ†• Unified enhanced RAG
    â”‚   â”‚   â”œâ”€â”€ reranker_service.py         # ğŸ†• Cross-encoder reranking
    â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py           # ğŸ†• BM25 keyword search
    â”‚   â”‚   â”œâ”€â”€ ensemble_retriever.py       # ğŸ†• Hybrid search orchestration
    â”‚   â”‚   â”œâ”€â”€ query_expander.py           # ğŸ†• Multi-query generation
    â”‚   â”‚   â”œâ”€â”€ corrective_rag.py           # ğŸ†• Self-grading RAG
    â”‚   â”‚   â”œâ”€â”€ web_search_fallback.py      # ğŸ†• External knowledge fallback
    â”‚   â”‚   â”œâ”€â”€ document_recovery_service.py # ğŸ†• Document repair/recovery
    â”‚   â”‚   â”œâ”€â”€ rag_service.py              # RAG retrieval orchestration
    â”‚   â”‚   â””â”€â”€ ollama_service.py           # LLM integration
    â”‚   â”‚
    â”‚   â”œâ”€â”€ utils/                 # Utility modules
    â”‚   â”‚   â”œâ”€â”€ async_web_scraper.py   # HTML content extraction
    â”‚   â”‚   â””â”€â”€ memory_manager.py      # ğŸ†• RAM/swap monitoring, adaptive batching
    â”‚   â”‚
    â”‚   â”œâ”€â”€ prompts/               # ğŸ†• LLM Prompt Templates
    â”‚   â”‚   â”œâ”€â”€ citation_template.py    # Citation-enforcing prompts
    â”‚   â”‚   â”œâ”€â”€ cot_template.py         # Chain-of-thought prompts
    â”‚   â”‚   â””â”€â”€ extractive_template.py  # Extractive QA prompts
    â”‚   â”‚
    â”‚   â”œâ”€â”€ scripts/               # Backend scripts
    â”‚   â”‚   â”œâ”€â”€ reembed_with_bge_m3.py   # BGE-M3 migration script
    â”‚   â”‚   â”œâ”€â”€ setup_postgres.sh        # Database setup
    â”‚   â”‚   â”œâ”€â”€ redis_health.py          # Redis monitoring
    â”‚   â”‚   â””â”€â”€ migrate_*.py             # Database migrations
    â”‚   â”‚
    â”‚   â”œâ”€â”€ docs/                  # Backend documentation
    â”‚   â”‚   â”œâ”€â”€ ENHANCED_SEARCH_INTEGRATION.md  # RAG features guide
    â”‚   â”‚   â”œâ”€â”€ README_ENHANCED_SEARCH.md       # Enhanced search overview
    â”‚   â”‚   â”œâ”€â”€ TESTING_GUIDE.md                # Testing documentation
    â”‚   â”‚   â”œâ”€â”€ VECTOR_SEARCH_OPTIMIZATION.md   # Performance tuning
    â”‚   â”‚   â””â”€â”€ redis/                          # Redis-specific docs
    â”‚   â”‚       â”œâ”€â”€ REDIS_DEPLOYMENT_STEPS.md
    â”‚   â”‚       â”œâ”€â”€ REDIS_FIX_SUMMARY.md
    â”‚   â”‚       â””â”€â”€ FIX_REDIS_PUBSUB.md
    â”‚   â”‚
    â”‚   â”œâ”€â”€ tests/                 # Backend tests
    â”‚   â”‚   â””â”€â”€ test_enhanced_search_integration.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€ main.py                # FastAPI application entry
    â”‚   â”œâ”€â”€ init_db.py             # Database initialization
    â”‚   â””â”€â”€ requirements.txt       # Backend dependencies
    â”‚
    â”œâ”€â”€ frontend/                   # React Frontend (TypeScript)
    â”‚   â”œâ”€â”€ src/
    â”‚   â”‚   â”œâ”€â”€ components/        # React components
    â”‚   â”‚   â”‚   â”œâ”€â”€ Auth/         # Login, Register
    â”‚   â”‚   â”‚   â”œâ”€â”€ Chat/         # Chat interface, conversations
    â”‚   â”‚   â”‚   â”œâ”€â”€ Documents/    # Upload, document list (ğŸ†• progress tracking)
    â”‚   â”‚   â”‚   â”œâ”€â”€ Layout/       # App layout, sidebar
    â”‚   â”‚   â”‚   â””â”€â”€ Settings/     # Model settings (ğŸ†• system prompt config)
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€â”€ contexts/          # React contexts
    â”‚   â”‚   â”‚   â”œâ”€â”€ AuthContext.tsx    # Auth state
    â”‚   â”‚   â”‚   â””â”€â”€ ChatContext.tsx    # Chat state + WebSocket
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€â”€ services/          # API services
    â”‚   â”‚   â”‚   â””â”€â”€ api.ts        # Axios API client
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€â”€ config/            # Configuration
    â”‚   â”‚   â”‚   â””â”€â”€ config.ts     # App settings
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€â”€ App.tsx            # Main app component
    â”‚   â”‚   â””â”€â”€ index.tsx          # React entry point
    â”‚   â”‚
    â”‚   â”œâ”€â”€ public/                # Static assets
    â”‚   â”œâ”€â”€ package.json           # Node dependencies
    â”‚   â””â”€â”€ README.md              # Frontend documentation
    â”‚
    â”œâ”€â”€ docs/                       # ğŸ“š Complete Documentation
    â”‚   â”œâ”€â”€ README.md              # Docs index
    â”‚   â”œâ”€â”€ README_ROOT.md         # Root docs overview
    â”‚   â”œâ”€â”€ BGE_M3_MIGRATION_GUIDE.md   # ğŸ†• BGE-M3 migration guide
    â”‚   â”œâ”€â”€ FEATURES_GUIDE.md      # ğŸ†• Complete features guide (consolidated)
    â”‚   â”œâ”€â”€ NETWORK_SETUP.md       # Network/LAN configuration
    â”‚   â”œâ”€â”€ PRODUCTION_DEPLOYMENT.md  # Production deployment
    â”‚   â”œâ”€â”€ REDIS_COMPLETE_GUIDE.md   # Redis setup & troubleshooting
    â”‚   â”œâ”€â”€ ADMIN_GUIDE.md         # Admin operations
    â”‚   â”œâ”€â”€ USER_GUIDE.md          # User manual
    â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md    # Docker deployment
    â”‚   â”œâ”€â”€ HANDOVER_DOCUMENT.md   # Project handover
    â”‚   â””â”€â”€ architecture/          # Architecture decisions
    â”‚
    â””â”€â”€ scripts/                    # Webapp utility scripts
        â”œâ”€â”€ deploy.sh              # Deployment automation
        â”œâ”€â”€ backup.sh              # Database backup
        â”œâ”€â”€ setup_ollama.sh        # Ollama setup
        â””â”€â”€ security_validator.py  # Security checks
```

**Note**: ğŸ†• indicates new features/files added in October 2024 reorganization.

## ğŸ¯ Main Components

### Backend (FastAPI + Python)
- **Authentication**: JWT tokens, bcrypt password hashing, session management
- **Document Processing**: Async upload, chunking, embedding generation
- **Vector Search**: PostgreSQL pgvector with HNSW indexing (50x faster)
- **RAG System**: TF-IDF retrieval, semantic search, context building
- **Chat API**: WebSocket real-time messaging, conversation management
- **LLM Integration**: Dual provider support (Ollama/vLLM) for Mistral, Llama2, CodeLlama with factory pattern

### Frontend (React + TypeScript)
- **Auth UI**: Login, registration, password validation
- **Document Manager**: Drag-and-drop upload, processing status, document list
- **Chat Interface**: Real-time messaging, conversation history, markdown rendering
- **Settings**: Model selection, RAG parameters, temperature control
- **Security**: Localhost-only validation, JWT authentication

### Database (PostgreSQL + pgvector)
- **Users**: Authentication, roles, sessions
- **Documents**: File metadata, processing status, chunks
- **Vectors**: Embeddings with pgvector extension, HNSW indexing
- **Conversations**: Chat history, messages, ratings

## ğŸ“Š Performance

- **Vector Search**: 50x faster with PostgreSQL pgvector vs Python
- **Document Processing**: Async processing with progress tracking
- **Real-Time Chat**: WebSocket streaming responses
- **Concurrent Users**: Multi-worker support (4 workers in production)
- **Connection Pooling**: Optimized database connections

## ğŸ›¡ï¸ Security Features

- **Local-Only**: No external API dependencies
- **JWT Authentication**: Secure token-based auth
- **Password Hashing**: bcrypt with secure rounds
- **Rate Limiting**: Prevent abuse
- **CORS Protection**: Localhost-only by default
- **Security Headers**: XSS, CSRF, clickjacking protection
- **Input Validation**: Pydantic models, type checking

## ğŸ“– Documentation

### Getting Started
- [Installation Guide](webapp/INSTALLATION_GUIDE.md) - Detailed setup
- [User Guide](webapp/docs/USER_GUIDE.md) - User manual
- [Admin Guide](webapp/docs/ADMIN_GUIDE.md) - Admin operations

### Features & Configuration
- [Features Guide](webapp/docs/FEATURES_GUIDE.md) - ğŸ†• Complete RAG features overview
- [BGE-M3 Migration Guide](webapp/docs/BGE_M3_MIGRATION_GUIDE.md) - ğŸ†• Embedding migration
- [Enhanced Search Integration](webapp/backend/docs/ENHANCED_SEARCH_INTEGRATION.md) - ğŸ†• Technical details
- [vLLM Complete Guide](webapp/docs/VLLM_GUIDE.md) - âœ… Production-ready multi-user LLM setup

### Deployment
- [Network Setup](webapp/docs/NETWORK_SETUP.md) - LAN access configuration
- [Production Deployment](webapp/docs/PRODUCTION_DEPLOYMENT.md) - Production guide
- [Redis Setup](webapp/docs/REDIS_COMPLETE_GUIDE.md) - ğŸ†• Redis configuration
- [Docker Deployment](webapp/docs/DEPLOYMENT_GUIDE.md) - Docker setup

### Development
- [Architecture](webapp/docs/architecture/) - System design
- [Testing Guide](webapp/backend/docs/TESTING_GUIDE.md) - Testing documentation
- [Handover Document](webapp/docs/HANDOVER_DOCUMENT.md) - Project overview

## ğŸ§ª Testing

### Backend Tests
```bash
cd webapp/backend
pytest tests/ -v
```

### Frontend Tests
```bash
cd webapp/frontend
npm test
```

### Integration Tests
```bash
cd webapp
./tests/test_backend.sh
```

## ğŸš€ Deployment

### Development
```bash
# Backend
cd webapp/backend && python main.py

# Frontend
cd webapp/frontend && npm start
```

### Production
```bash
# Update configuration
cd webapp/backend
cp .env.example .env
# Edit .env: DEBUG=False, SECRET_KEY=<new-key>

# Build frontend
cd ../frontend
npm run build
cp -r build/* ../backend/static/

# Run backend (auto-serves frontend)
cd ../backend
python main.py  # 4 workers, docs disabled, production mode
```

See [PRODUCTION_DEPLOYMENT.md](docs/PRODUCTION_DEPLOYMENT.md) for complete guide.

## ğŸ”§ Configuration

### Backend (.env)
```bash
# Database
DATABASE_URL=postgresql://rag_user:password@localhost:5432/rag_database
DEBUG=False  # Set to True for development

# Security
SECRET_KEY=<generate-secure-key>
ACCESS_TOKEN_EXPIRE_MINUTES=30

# LLM Provider (choose one)
LLM_PROVIDER=ollama  # Options: "ollama" (default), "vllm"

# Ollama (default)
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=mistral

# vLLM (optional - for multi-user production)
VLLM_BASE_URL=http://localhost:8001
VLLM_GPU_COUNT=1
VLLM_TENSOR_PARALLEL_SIZE=1

# Server
HOST=127.0.0.1
PORT=8000
```

### Frontend (.env)
```bash
REACT_APP_API_URL=http://127.0.0.1:8000
REACT_APP_WS_URL=http://127.0.0.1:8000
PORT=3000
```

## ğŸ—„ï¸ Database Setup

### PostgreSQL with pgvector
```bash
# Install PostgreSQL and pgvector
sudo apt-get install postgresql postgresql-contrib
sudo apt-get install postgresql-14-pgvector

# Create database and user
sudo -u postgres psql
CREATE DATABASE rag_database;
CREATE USER rag_user WITH PASSWORD 'secure_password';
GRANT ALL PRIVILEGES ON DATABASE rag_database TO rag_user;

# Enable pgvector extension
\c rag_database
CREATE EXTENSION IF NOT EXISTS vector;
```

See [webapp setup scripts](webapp/backend/scripts/) for automated setup.

## ğŸ¤– LLM Provider Setup

The RAG system supports two LLM providers with easy switching via configuration:

### Ollama (Default)

**Best for:** Development, single-user, simple setup

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama
ollama serve

# Pull models
ollama pull mistral
ollama pull llama2
ollama pull codellama
```

### vLLM (Production, Multi-User)

**Best for:** Production deployment, 5+ concurrent users, multi-GPU servers

```bash
# Docker (recommended)
docker pull vllm/vllm-openai:latest
docker run --gpus all -p 8001:8000 --ipc=host \
    vllm/vllm-openai:latest \
    --model mistralai/Mistral-7B-Instruct-v0.2

# Or native installation
pip install vllm==0.11.0
python -m vllm.entrypoints.openai.api_server \
    --model mistralai/Mistral-7B-Instruct-v0.2 \
    --port 8001
```

**Switching Providers (No Code Changes):**

```bash
# Use vLLM in .env
LLM_PROVIDER=vllm
VLLM_BASE_URL=http://localhost:8001

# Restart backend - that's it!
```

**ğŸ“š Complete Guide:** See [webapp/docs/VLLM_GUIDE.md](webapp/docs/VLLM_GUIDE.md) for comprehensive documentation on:
- Architecture and factory pattern
- Installation methods (Docker, native, conda)
- Configuration and usage
- Performance tuning and multi-GPU setup
- Troubleshooting and migration guide

**Features:**
- âœ… Simple one-command setup
- âœ… Automatic model management
- âœ… Good for development and low-concurrency
- âœ… Works on systems with limited GPU resources

---

### vLLM (Advanced - Optional)

**Best for:** Production, multi-user (5+), multi-GPU servers

**Why vLLM?**
- ğŸš€ **8-100x faster** for concurrent requests
- ğŸ”¥ **Parallel batching** (vs Ollama's serial processing)
- ğŸ’ª **Multi-GPU tensor parallelism**
- âš¡ **Production-scale throughput**

**Quick Start (Docker):**
```bash
# Pull official vLLM image
docker pull vllm/vllm-openai:latest

# Run vLLM server (single GPU)
docker run --gpus all -p 8001:8000 --ipc=host \
    vllm/vllm-openai:latest \
    --model mistralai/Mistral-7B-Instruct-v0.2 \
    --host 0.0.0.0 --port 8000

# Switch to vLLM in .env
LLM_PROVIDER=vllm
VLLM_BASE_URL=http://localhost:8001
```

**Quick Start:**
```bash
# Use the provided script (recommended)
bash webapp/scripts/start_vllm.sh

# Or see complete documentation
# webapp/docs/VLLM_GUIDE.md
```

**Comparison:**

| Feature | Ollama | vLLM |
|---------|--------|------|
| Setup | Simple (one command) | Easy (provided script) âœ… |
| Single User | Fast âœ… | Fast âœ… |
| 10 Concurrent Users | Slow (serialized) | **8-10x faster** ğŸš€ |
| Multi-GPU | Limited | Excellent âœ… |
| Model Management | Automatic | Manual (Hugging Face) |
| Production Ready | Development | âœ… Production |

**Switching Providers:**
```bash
# No code changes required! Just update .env

# Use Ollama (default)
LLM_PROVIDER=ollama

# Use vLLM (production)
LLM_PROVIDER=vllm
```

For detailed vLLM setup, performance tuning, and multi-GPU configuration, see [VLLM_GUIDE.md](webapp/docs/VLLM_GUIDE.md).

## ğŸ“ API Endpoints

### Authentication
- `POST /api/auth/register` - Register new user
- `POST /api/auth/login` - Login user
- `POST /api/auth/logout` - Logout user
- `GET /api/auth/me` - Get current user

### Documents
- `POST /api/documents/upload` - Upload document
- `GET /api/documents` - List documents
- `DELETE /api/documents/{id}` - Delete document
- `GET /api/documents/{id}/chunks` - Get document chunks

### Chat
- `WebSocket /socket.io` - Real-time chat
- `GET /api/conversations` - List conversations
- `POST /api/conversations` - Create conversation
- `GET /api/conversations/{id}/messages` - Get messages

### Models
- `GET /api/models` - List available Ollama models
- `GET /api/models/default` - Get default model

## ğŸ¨ Frontend Features

- **Authentication**: Secure login/register with validation
- **Document Upload**: Drag-and-drop with progress tracking
- **Chat Interface**: Real-time messaging with markdown rendering
- **Conversation Management**: Create, switch, delete conversations
- **Model Selection**: Choose Ollama models
- **RAG Settings**: Configure temperature, top_k, RAG mode
- **Responsive Design**: Works on desktop and mobile

## ğŸŒ Network Access

For LAN access (other devices on your network):

1. Update `webapp/backend/.env`: `HOST=0.0.0.0`
2. Update `webapp/frontend/.env`: `REACT_APP_API_URL=http://<your-ip>:8000`
3. Configure firewall to allow ports 3000, 8000

See [NETWORK_SETUP.md](docs/NETWORK_SETUP.md) for complete guide.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is open source. See the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **FastAPI**: Modern Python web framework
- **React**: UI library
- **PostgreSQL**: Powerful database with vector support
- **Ollama**: Local LLM platform
- **Material-UI**: React component library
- **Socket.IO**: Real-time communication

## ğŸ“ Support

- ğŸ“– **Documentation**: See [webapp/docs/](webapp/docs/)
- ğŸ› **Issues**: Report on GitHub
- ğŸ’¬ **Questions**: GitHub Discussions
- ğŸ“§ **Contact**: Reach out to maintainers

---

**Note**: The `archive/old_rag_system/` folder contains the previous standalone RAG library for reference. The current production application is in `webapp/`.
