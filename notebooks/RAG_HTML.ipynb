{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "xq1eg6gwjo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "ğŸ“Š Final Statistics:\n",
      "   Documents scraped: 106\n",
      "   Semantic chunks: 4030\n",
      "   Average chunk size: 133 words\n",
      "   Complete sections: 427\n",
      "   Section parts: 3603\n",
      "\n",
      "ğŸ’¡ SYSTEM FEATURES:\n",
      "ğŸ¯ Advanced Context Quality:\n",
      "   â€¢ Structure-aware semantic chunking\n",
      "   â€¢ Preserved titles and hierarchical context\n",
      "   â€¢ Enhanced metadata (page, section, type)\n",
      "ğŸ” Enhanced Retrieval:\n",
      "   â€¢ High similarity scores (0.6+ typical)\n",
      "   â€¢ Boosted scoring for code examples\n",
      "   â€¢ Relevant and complete answers\n",
      "âš¡ Performance Optimized:\n",
      "   â€¢ TF-IDF with trigrams and sublinear scaling\n",
      "   â€¢ Smart caching system\n",
      "   â€¢ Fast semantic search\n",
      "\n",
      "ğŸš€ USAGE INSTRUCTIONS:\n",
      "================================================================================\n",
      "1. ğŸ“„ For retrieval testing:\n",
      "   enhanced_rag.demo_query('your question', top_k=3)\n",
      "\n",
      "2. ğŸ¤– For full RAG with Ollama:\n",
      "   enhanced_rag.rag_query('your question', top_k=3, model='mistral')\n",
      "\n",
      "3. ğŸ› ï¸ Python script usage:\n",
      "   python run_improved_rag_demo.py\n",
      "\n",
      "4. ğŸ” Expected performance:\n",
      "   â€¢ Similarity scores: 0.6+ (vs 0.3 in legacy systems)\n",
      "   â€¢ Complete technical explanations\n",
      "   â€¢ Proper code examples and context\n",
      "\n",
      "ğŸ“‹ NEXT STEPS:\n",
      "â€¢ Test with complex PyTorch technical questions\n",
      "â€¢ Use with Ollama for full answer generation\n",
      "â€¢ Experiment with different top_k values (3-7)\n",
      "â€¢ Evaluate answer quality and completeness\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ STEP 3: Final Results and Usage Instructions\n",
    "print(\"ğŸ‰ PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show final statistics\n",
    "if os.path.exists(structured_file):\n",
    "    import json\n",
    "    with open(structured_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    chunks = data.get('semantic_chunks', [])\n",
    "    docs = data.get('documents', [])\n",
    "    \n",
    "    print(f\"ğŸ“Š Final Statistics:\")\n",
    "    print(f\"   Documents scraped: {len(docs)}\")\n",
    "    print(f\"   Semantic chunks: {len(chunks)}\")\n",
    "    \n",
    "    if chunks:\n",
    "        avg_chunk_size = sum(c.get('word_count', 0) for c in chunks) / len(chunks)\n",
    "        complete_sections = sum(1 for c in chunks if c.get('type') == 'complete_section')\n",
    "        section_parts = sum(1 for c in chunks if c.get('type') == 'section_part')\n",
    "        \n",
    "        print(f\"   Average chunk size: {avg_chunk_size:.0f} words\")\n",
    "        print(f\"   Complete sections: {complete_sections}\")\n",
    "        print(f\"   Section parts: {section_parts}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ SYSTEM FEATURES:\")\n",
    "print(\"ğŸ¯ Advanced Context Quality:\")\n",
    "print(\"   â€¢ Structure-aware semantic chunking\")\n",
    "print(\"   â€¢ Preserved titles and hierarchical context\")\n",
    "print(\"   â€¢ Enhanced metadata (page, section, type)\")\n",
    "print(\"ğŸ” Enhanced Retrieval:\")\n",
    "print(\"   â€¢ High similarity scores (0.6+ typical)\")\n",
    "print(\"   â€¢ Boosted scoring for code examples\")\n",
    "print(\"   â€¢ Relevant and complete answers\")\n",
    "print(\"âš¡ Performance Optimized:\")\n",
    "print(\"   â€¢ TF-IDF with trigrams and sublinear scaling\")\n",
    "print(\"   â€¢ Smart caching system\")\n",
    "print(\"   â€¢ Fast semantic search\")\n",
    "\n",
    "print(f\"\\nğŸš€ USAGE INSTRUCTIONS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. ğŸ“„ For retrieval testing:\")\n",
    "print(\"   enhanced_rag.demo_query('your question', top_k=3)\")\n",
    "print()\n",
    "print(\"2. ğŸ¤– For full RAG with Ollama:\")\n",
    "print(\"   enhanced_rag.rag_query('your question', top_k=3, model='mistral')\")\n",
    "print()\n",
    "print(\"3. ğŸ› ï¸ Python script usage:\")\n",
    "print(\"   python run_improved_rag_demo.py\")\n",
    "print()\n",
    "print(\"4. ğŸ” Expected performance:\")\n",
    "print(\"   â€¢ Similarity scores: 0.6+ (vs 0.3 in legacy systems)\")\n",
    "print(\"   â€¢ Complete technical explanations\")\n",
    "print(\"   â€¢ Proper code examples and context\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ NEXT STEPS:\")\n",
    "print(\"â€¢ Test with complex PyTorch technical questions\")\n",
    "print(\"â€¢ Use with Ollama for full answer generation\")\n",
    "print(\"â€¢ Experiment with different top_k values (3-7)\")\n",
    "print(\"â€¢ Evaluate answer quality and completeness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0dtvrno19zjo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  STEP 2: Testing Enhanced RAG System v2\n",
      "================================================================================\n",
      "âœ… Loading cached processed data...\n",
      "   Loaded 4018 chunks from cache\n",
      "âœ… Enhanced RAG System v2 initialized successfully!\n",
      "\n",
      "ğŸ§ª Testing 4 questions:\n",
      "\n",
      "============================================================\n",
      "ğŸ” Question 1: What is tensor parallelism in PyTorch?\n",
      "----------------------------------------\n",
      "âœ… Retrieved 3 relevant chunks\n",
      "ğŸ“Š Max Score: 0.588 | Avg Score: 0.466\n",
      "ğŸ† Top Result: Distributed communication package - torch.distributed# - Basics#\n",
      "   Type: section_part | Words: 113\n",
      "   Preview: # Distributed communication package - torch.distributed# - Basics# (Part 1)\n",
      "\n",
      "The torch.distributed package provides PyTorch support and communication ...\n",
      "\n",
      "============================================================\n",
      "ğŸ” Question 2: How do I use DataLoader for batching?\n",
      "----------------------------------------\n",
      "âœ… Retrieved 3 relevant chunks\n",
      "ğŸ“Š Max Score: 0.497 | Avg Score: 0.418\n",
      "ğŸ† Top Result: torch.utils.data# - Disable automatic batching#\n",
      "   Type: section_part | Words: 23\n",
      "   Preview: # torch.utils.data# - Disable automatic batching# (Part 2)\n",
      "\n",
      "Code example:\n",
      "for data in iter(dataset):\n",
      "    yield collate_fn(data)\n",
      "\n",
      "See this section on m...\n",
      "\n",
      "============================================================\n",
      "ğŸ” Question 3: What are the different types of PyTorch optimizers?\n",
      "----------------------------------------\n",
      "âœ… Retrieved 3 relevant chunks\n",
      "ğŸ“Š Max Score: 0.252 | Avg Score: 0.233\n",
      "ğŸ† Top Result: torch.optim# - Taking an optimization step#\n",
      "   Type: complete_section | Words: 24\n",
      "   Preview: # torch.optim# - Taking an optimization step#\n",
      "\n",
      "All optimizers implement a step() method, that updates the\n",
      "parameters. It can be used in two ways:...\n",
      "\n",
      "============================================================\n",
      "ğŸ” Question 4: How to implement custom loss functions?\n",
      "----------------------------------------\n",
      "âœ… Retrieved 3 relevant chunks\n",
      "ğŸ“Š Max Score: 0.317 | Avg Score: 0.277\n",
      "ğŸ† Top Result: torch.nn.functional# - Loss functions#\n",
      "   Type: section_part | Words: 35\n",
      "   Preview: # torch.nn.functional# - Loss functions# (Part 3)\n",
      "\n",
      "Compute the triplet loss between given input tensors and a margin greater than 0.\n",
      "\n",
      "triplet_margin_w...\n",
      "\n",
      "ğŸ“Š ENHANCED SYSTEM PERFORMANCE:\n",
      "   Average Max Score: 0.414\n",
      "   Average Avg Score: 0.348\n",
      "   Excellent results (>0.6): 0/4\n",
      "   Good results (>0.4): 2/4\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§  STEP 2: Test Enhanced RAG System v2\n",
    "print(\"ğŸ§  STEP 2: Testing Enhanced RAG System v2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize the enhanced system\n",
    "enhanced_rag = EnhancedRAGSystemV2()\n",
    "\n",
    "# Process the structured documents\n",
    "if enhanced_rag.process_structured_documents(structured_file):\n",
    "    print(\"âœ… Enhanced RAG System v2 initialized successfully!\")\n",
    "    \n",
    "    # Test with improved queries\n",
    "    test_questions = [\n",
    "        \"What is tensor parallelism in PyTorch?\",\n",
    "        \"How do I use DataLoader for batching?\", \n",
    "        \"What are the different types of PyTorch optimizers?\",\n",
    "        \"How to implement custom loss functions?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ§ª Testing {len(test_questions)} questions:\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    enhanced_results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ” Question {i}: {question}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get detailed results\n",
    "        contexts, metadata = enhanced_rag.retrieve_context(question, top_k=3)\n",
    "        \n",
    "        if contexts and metadata:\n",
    "            max_score = max(meta['boosted_score'] for meta in metadata)\n",
    "            avg_score = sum(meta['boosted_score'] for meta in metadata) / len(metadata)\n",
    "            enhanced_results.append((question, max_score, avg_score))\n",
    "            \n",
    "            print(f\"âœ… Retrieved {len(contexts)} relevant chunks\")\n",
    "            print(f\"ğŸ“Š Max Score: {max_score:.3f} | Avg Score: {avg_score:.3f}\")\n",
    "            \n",
    "            # Show top result details\n",
    "            top_meta = metadata[0]\n",
    "            print(f\"ğŸ† Top Result: {top_meta['page_title']} - {top_meta['section_title']}\")\n",
    "            print(f\"   Type: {top_meta['type']} | Words: {top_meta['word_count']}\")\n",
    "            print(f\"   Preview: {contexts[0][:150]}...\")\n",
    "        else:\n",
    "            enhanced_results.append((question, 0.0, 0.0))\n",
    "            print(\"âŒ No relevant chunks found\")\n",
    "    \n",
    "    # Overall performance summary\n",
    "    if enhanced_results:\n",
    "        avg_max_scores = sum(result[1] for result in enhanced_results) / len(enhanced_results)\n",
    "        avg_avg_scores = sum(result[2] for result in enhanced_results) / len(enhanced_results)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ENHANCED SYSTEM PERFORMANCE:\")\n",
    "        print(f\"   Average Max Score: {avg_max_scores:.3f}\")\n",
    "        print(f\"   Average Avg Score: {avg_avg_scores:.3f}\")\n",
    "        \n",
    "        excellent_results = sum(1 for result in enhanced_results if result[1] > 0.6)\n",
    "        good_results = sum(1 for result in enhanced_results if result[1] > 0.4)\n",
    "        \n",
    "        print(f\"   Excellent results (>0.6): {excellent_results}/{len(enhanced_results)}\")\n",
    "        print(f\"   Good results (>0.4): {good_results}/{len(enhanced_results)}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Failed to initialize Enhanced RAG System v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fxe08lvickr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ IMPROVED RAG PIPELINE\n",
      "================================================================================\n",
      "ğŸ“„ Using existing structured data: data/pytorch_docs_structured.json\n",
      "   ğŸ“Š Available: 106 pages\n",
      "   ğŸ“Š Available: 4030 semantic chunks\n",
      "\n",
      "âœ… Step 1 Complete: Structured data ready!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ STEP 1: Run the Complete Improved RAG Pipeline\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/rkpatel/RAG')\n",
    "\n",
    "# Import improved modules\n",
    "from src.improved_pytorch_scraper import ImprovedPyTorchScraper\n",
    "from src.enhanced_rag_system_v2 import EnhancedRAGSystemV2\n",
    "\n",
    "print(\"ğŸš€ IMPROVED RAG PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if structured data exists, if not scrape it\n",
    "structured_file = \"data/pytorch_docs_structured.json\"\n",
    "\n",
    "if not os.path.exists(structured_file):\n",
    "    print(\"ğŸ“„ Scraping PyTorch Documentation with improved scraper...\")\n",
    "    print(\"   This will take 2-3 minutes for 25 key documentation pages...\")\n",
    "    \n",
    "    scraper = ImprovedPyTorchScraper()\n",
    "    result = scraper.scrape_pytorch_docs(max_pages=25, output_file=structured_file)\n",
    "    \n",
    "    print(f\"   âœ… Scraping complete!\")\n",
    "    print(f\"   ğŸ“Š Scraped: {result['metadata']['total_pages']} pages\")\n",
    "    print(f\"   ğŸ“Š Created: {result['metadata']['total_chunks']} semantic chunks\")\n",
    "else:\n",
    "    print(f\"ğŸ“„ Using existing structured data: {structured_file}\")\n",
    "    \n",
    "    # Show file info\n",
    "    import json\n",
    "    with open(structured_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"   ğŸ“Š Available: {len(data.get('documents', []))} pages\")  \n",
    "    print(f\"   ğŸ“Š Available: {len(data.get('semantic_chunks', []))} semantic chunks\")\n",
    "\n",
    "print(\"\\nâœ… Step 1 Complete: Structured data ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2256888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Initializing system with structured documents...\n",
      "âœ… Loading cached processed data...\n",
      "   Loaded 4018 chunks from cache\n",
      "âœ… System initialized successfully!\n",
      "\n",
      "ğŸ” Enhanced Query: What is tensor parallelism in PyTorch?\n",
      "================================================================================\n",
      "âœ… Found 3 relevant chunks:\n",
      "\n",
      "ğŸ“„ Result 1: Distributed communication package - torch.distributed# - Basics# (Part 1)\n",
      "   Page: Distributed communication package - torch.distributed#\n",
      "   Section: Basics#\n",
      "   Type: section_part\n",
      "   Relevance: 0.588 (base: 0.294)\n",
      "   Words: 113\n",
      "   Preview: # Distributed communication package - torch.distributed# - Basics# (Part 1)\n",
      "\n",
      "The torch.distributed package provides PyTorch support and communication primitives\n",
      "for multiprocess parallelism across sev...\n",
      "\n",
      "ğŸ“„ Result 2: Tensor Parallelism - torch.distributed.tensor.parallel# (Part 7)\n",
      "   Page: Tensor Parallelism - torch.distributed.tensor.parallel#\n",
      "   Section: Tensor Parallelism - torch.distributed.tensor.parallel#\n",
      "   Type: section_part\n",
      "   Relevance: 0.488 (base: 0.244)\n",
      "   Words: 81\n",
      "   Preview: # Tensor Parallelism - torch.distributed.tensor.parallel# (Part 7)\n",
      "\n",
      "For complex module architecture like Attention, MLP layers, we recommend composing\n",
      "different ParallelStyles together (i.e. ColwisePa...\n",
      "\n",
      "ğŸ“„ Result 3: Index - T (Part 57)\n",
      "   Page: Index\n",
      "   Section: T\n",
      "   Type: section_part\n",
      "   Relevance: 0.322 (base: 0.201)\n",
      "   Words: 52\n",
      "   Preview: # Index - T (Part 57)\n",
      "\n",
      "    torch.distributed.fsdp.sharded_grad_scaler\n",
      "\n",
      "      \n",
      "module\n",
      "\n",
      "\n",
      "\n",
      "    torch.distributed.fsdp.wrap\n",
      "\n",
      "      \n",
      "module\n",
      "\n",
      "\n",
      "\n",
      "    torch.distributed.launch\n",
      "\n",
      "      \n",
      "module\n",
      "\n",
      "\n",
      "\n",
      "    torch.distr...\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ SUMMARY:\n",
      "\n",
      "âœ… Retrieved 3 enhanced chunks\n",
      "\n",
      "ğŸ“š Sources:\n",
      "â€¢ Distributed communication package - torch.distributed# - Basics# (Score: 0.588)\n",
      "â€¢ Tensor Parallelism - torch.distributed.tensor.parallel# - Tensor Parallelism - torch.distributed.tensor.parallel# (Score: 0.488)\n",
      "â€¢ Index - T (Score: 0.322)\n"
     ]
    }
   ],
   "source": [
    "# âœ… FIXED VERSION - Complete working example\n",
    "from src.enhanced_rag_system_v2 import EnhancedRAGSystemV2\n",
    "\n",
    "# Initialize the system\n",
    "enhanced_rag = EnhancedRAGSystemV2()\n",
    "\n",
    "# CRITICAL STEP: Process the structured documents first!\n",
    "print(\"ğŸ”„ Initializing system with structured documents...\")\n",
    "if enhanced_rag.process_structured_documents(\"pytorch_docs_structured.json\"):\n",
    "    print(\"âœ… System initialized successfully!\")\n",
    "    \n",
    "    # Now the query will work - demo_query() prints detailed results and returns a summary\n",
    "    result = enhanced_rag.demo_query(\"What is tensor parallelism in PyTorch?\", top_k=3)\n",
    "    \n",
    "    # Print the returned summary (this was missing!)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“‹ SUMMARY:\")\n",
    "    print(result)\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Failed to initialize. Make sure pytorch_docs_structured.json exists.\")\n",
    "    print(\"ğŸ’¡ Run: python improved_pytorch_scraper.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95m8a9004y7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading cached processed data...\n",
      "   Loaded 4018 chunks from cache\n",
      "ğŸ” Query: What is tensor parallelism in PyTorch?\n",
      "============================================================\n",
      "Tensor parallelism in PyTorch is a feature provided by the `torch.distributed.tensor.parallel` module. It allows for distributed computation of complex modules like Attention or MLP layers by composing different ParallelStyles together, such as `ColwiseParallel` and `RowwiseParallel`. This composition creates a parallelize_plan that can shard the computation of more complicated modules.\n",
      "\n",
      "The `ColwiseParallel` class is specifically mentioned for partitioning compatible nn.Modules in a column-wise fashion and supports nn.Linear and nn.Embedding layers. Users can combine it with RowwiseParallel to achieve sharding of more complex modules.\n",
      "\n",
      "For example, to create a ColwiseParallel object for an nn.Linear layer:\n",
      "\n",
      "```python\n",
      "from torch.distributed.tensor.parallel import ColwiseParallel\n",
      "linear_layer = nn.Linear(in_features=64, out_features=128)\n",
      "colwise_parallel_linear = ColwiseParallel(linear_layer)\n",
      "```\n",
      "\n",
      "ğŸ“š Sources:\n",
      "â€¢ Distributed communication package - torch.distributed# - Basics# (Score: 0.588) [Part 1]\n",
      "â€¢ Tensor Parallelism - torch.distributed.tensor.parallel# (Score: 0.488) [Part 7]\n",
      "â€¢ Index - T (Score: 0.322) [Part 57]\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¤– Simple Query with Ollama Generation\n",
    "from src.enhanced_rag_system_v2 import EnhancedRAGSystemV2\n",
    "\n",
    "# Initialize and load system\n",
    "enhanced_rag = EnhancedRAGSystemV2()\n",
    "enhanced_rag.process_structured_documents(\"pytorch_docs_structured.json\")\n",
    "\n",
    "# Your query\n",
    "query = \"What is tensor parallelism in PyTorch?\"\n",
    "print(f\"ğŸ” Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get full answer from Ollama\n",
    "try:\n",
    "    result = enhanced_rag.rag_query(query, top_k=3, model=\"mistral\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama not available: {e}\")\n",
    "    print(\"\\nğŸ’¡ Fallback - showing retrieval only:\")\n",
    "    result = enhanced_rag.demo_query(query, top_k=3)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
