#!/usr/bin/env python3
"""
Generic RAG System for Any Website
Complete RAG system with web scraping and enhanced retrieval capabilities
"""

import json
import pickle
import os
import requests
import time
from typing import List, Dict, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

try:
    from .web_scraper import WebScraper
    from .async_web_scraper import AsyncWebScraper, scrape_website_fast, ScrapingConfig
except ImportError:
    from web_scraper import WebScraper
    from async_web_scraper import AsyncWebScraper, scrape_website_fast, ScrapingConfig

import asyncio


class RAGSystem:
    """Complete RAG system that can work with any website"""

    def __init__(self, chunk_size: int = 1200, overlap: int = 100, use_async: bool = False):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.chunks = []
        self.chunk_metadata = []
        self.vectorizer = None
        self.tfidf_matrix = None
        self.structured_data = None
        self.scraper = WebScraper()
        self.use_async = use_async

    def scrape_and_process_website(self, start_urls: List[str],
                                 max_pages: int = 30,
                                 output_file: str = "data/website_docs.json",
                                 same_domain_only: bool = True,
                                 max_depth: int = 2,
                                 use_cache: bool = True) -> bool:
        """Scrape a website and process it for RAG"""

        # Check if we already have cached data and processing cache
        cache_file = output_file.replace('.json', '_cache.pkl')

        if use_cache and os.path.exists(output_file) and os.path.exists(cache_file):
            # Load the processed data directly
            success = self.process_structured_documents(output_file)

            if success:
                print("âœ… Using cached data - ready to query!")
                return True

        elif use_cache and os.path.exists(output_file):
            # Process existing scraped data
            success = self.process_structured_documents(output_file)

            if success:
                print("âœ… Existing data processed successfully!")
                return True

        # If we get here, we need to scrape

        # Scrape the website
        scraping_result = self.scraper.scrape_website(
            start_urls=start_urls,
            max_pages=max_pages,
            output_file=output_file,
            same_domain_only=same_domain_only,
            max_depth=max_depth
        )

        if not scraping_result:
            return False

        # Process the newly scraped data
        success = self.process_structured_documents(output_file)

        if success:
            print("âœ… Website scraped and processed successfully!")

        return success

    async def process_local_files_async(self, file_paths: List[str],
                                      output_file: str = "data/local_docs_async.json",
                                      concurrent_limit: int = 6,
                                      use_cache: bool = True) -> bool:
        """Process local HTML files asynchronously for RAG"""

        # Check cache first
        cache_file = output_file.replace('.json', '_cache.pkl')

        if use_cache and os.path.exists(output_file) and os.path.exists(cache_file):
            success = self.process_structured_documents(output_file)
            if success:
                print("âœ… Using cached data - ready to query!")
                return True

        # Perform async local file processing
        start_time = time.time()

        try:
            from .async_web_scraper import process_local_files_fast

            results = await process_local_files_fast(
                file_paths=file_paths,
                output_file=output_file,
                concurrent_limit=concurrent_limit
            )

            duration = time.time() - start_time
            metadata = results.get("metadata", {})

            # Process the data for RAG
            success = self.process_structured_documents(output_file)

            if success:
                print("âœ… Async local files processed and ready for RAG!")

            return success

        except Exception as e:
            return False

    async def scrape_and_process_website_async(self, start_urls: List[str],
                                             max_pages: int = 30,
                                             output_file: str = "data/website_docs_async.json",
                                             concurrent_limit: int = 8,
                                             requests_per_second: float = 10.0,
                                             use_cache: bool = True) -> bool:
        """Async version for high-performance scraping"""

        # Check cache first
        cache_file = output_file.replace('.json', '_cache.pkl')

        if use_cache and os.path.exists(output_file) and os.path.exists(cache_file):
            success = self.process_structured_documents(output_file)
            if success:
                print("âœ… Using cached data - ready to query!")
                return True

        # Perform async scraping
        start_time = time.time()

        try:
            results = await scrape_website_fast(
                start_urls=start_urls,
                max_pages=max_pages,
                concurrent_limit=concurrent_limit,
                requests_per_second=requests_per_second,
                output_file=output_file
            )

            duration = time.time() - start_time
            metadata = results.get("metadata", {})

            # Process the scraped data
            success = self.process_structured_documents(output_file)

            if success:
                print("âœ… Async website scraped and processed successfully!")

            return success

        except Exception as e:
            return False

    async def process_mixed_sources_async(self, web_urls: List[str] = None,
                                        local_files: List[str] = None,
                                        output_file: str = "data/mixed_docs_async.json",
                                        max_pages: int = 30,
                                        concurrent_limit: int = 6,
                                        use_cache: bool = True) -> bool:
        """Process both web URLs and local HTML files asynchronously in a single operation"""

        # Check cache first
        cache_file = output_file.replace('.json', '_cache.pkl')

        if use_cache and os.path.exists(output_file) and os.path.exists(cache_file):
            success = self.process_structured_documents(output_file)
            if success:
                print("âœ… Using cached data - ready to query!")
                return True

        start_time = time.time()
        all_structured_docs = []

        try:
            # Process web URLs if provided
            if web_urls:
                web_success = await self.scrape_and_process_website_async(
                    start_urls=web_urls,
                    max_pages=max_pages,
                    output_file=f"{output_file}_web_temp.json",
                    concurrent_limit=concurrent_limit,
                    use_cache=False  # Don't cache temp files
                )

                if web_success:
                    # Load web data and extract documents
                    with open(f"{output_file}_web_temp.json", 'r') as f:
                        web_data = json.load(f)
                    all_structured_docs.extend(web_data.get('documents', []))
                else:
                    pass

            # Process local files if provided
            if local_files:
                local_success = await self.process_local_files_async(
                    file_paths=local_files,
                    output_file=f"{output_file}_local_temp.json",
                    concurrent_limit=concurrent_limit,
                    use_cache=False  # Don't cache temp files
                )

                if local_success:
                    # Load local data and extract documents
                    with open(f"{output_file}_local_temp.json", 'r') as f:
                        local_data = json.load(f)
                    all_structured_docs.extend(local_data.get('documents', []))
                else:
                    pass

            if not all_structured_docs:
                return False

            # Create combined semantic chunks using sync scraper method
            from .web_scraper import WebScraper
            scraper = WebScraper()
            semantic_chunks = scraper.create_semantic_chunks(all_structured_docs)

            # Create combined output
            combined_output = {
                "metadata": {
                    "processing_timestamp": time.time(),
                    "source_type": "mixed_async",
                    "web_urls": web_urls or [],
                    "local_files": local_files or [],
                    "total_documents": len(all_structured_docs),
                    "total_chunks": len(semantic_chunks),
                    "processing_time": time.time() - start_time,
                    "domains": list(set(doc.get("domain", "") for doc in all_structured_docs))
                },
                "documents": all_structured_docs,
                "semantic_chunks": semantic_chunks
            }

            # Save combined data
            os.makedirs(os.path.dirname(output_file), exist_ok=True)

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(combined_output, f, indent=2, ensure_ascii=False)

            # Create text file for compatibility
            text_file = output_file.replace('.json', '.txt')
            with open(text_file, 'w', encoding='utf-8') as f:
                for chunk in semantic_chunks:
                    f.write(f"{chunk['text']}\n\n{'='*80}\n\n")

            # Clean up temporary files
            for temp_file in [f"{output_file}_web_temp.json", f"{output_file}_local_temp.json"]:
                if os.path.exists(temp_file):
                    os.remove(temp_file)

            # Process the combined data for RAG
            success = self.process_structured_documents(output_file)

            if success:
                print(f"âœ… Mixed async processing complete! Docs: {len(all_structured_docs)}, Chunks: {len(semantic_chunks)}")

            return success

        except Exception as e:
            return False

    def clear_cache(self, output_file: str = "data/website_docs.json") -> bool:
        """Clear cached data files to force re-scraping"""

        cache_file = output_file.replace('.json', '_cache.pkl')
        text_file = output_file.replace('.json', '.txt')

        files_removed = []

        for file_path in [output_file, cache_file, text_file]:
            if os.path.exists(file_path):
                try:
                    os.remove(file_path)
                    files_removed.append(file_path)
                except Exception as e:
                    pass

        if files_removed:
            return True
        else:
            return False

    def load_structured_data(self, file_path: str) -> bool:
        """Load structured data from JSON file"""

        if not os.path.exists(file_path):
            return False

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                self.structured_data = json.load(f)

            semantic_chunks = self.structured_data.get('semantic_chunks', [])
            return True

        except Exception as e:
            return False

    def process_structured_documents(self, file_path: str = None) -> bool:
        """Process structured documents and build TF-IDF index"""

        # Load data if file path provided
        if file_path:
            if not self.load_structured_data(file_path):
                return False

        if not self.structured_data:
            return False

        semantic_chunks = self.structured_data.get('semantic_chunks', [])

        if not semantic_chunks:
            return False

        # Check for cached processed data
        cache_file = file_path.replace('.json', '_cache.pkl') if file_path else 'rag_cache.pkl'

        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)

                self.chunks = cache_data['chunks']
                self.chunk_metadata = cache_data['chunk_metadata']
                self.vectorizer = cache_data['vectorizer']
                self.tfidf_matrix = cache_data['tfidf_matrix']

                return True

            except Exception as e:
                pass

        # Process chunks

        self.chunks = []
        self.chunk_metadata = []

        for chunk_data in semantic_chunks:
            chunk_text = chunk_data.get('text', '')
            if len(chunk_text.strip()) < 10:
                continue

            self.chunks.append(chunk_text)

            # Extract metadata
            metadata = {
                'title': chunk_data.get('title', 'Unknown'),
                'section_title': chunk_data.get('section_title', 'Unknown'),
                'page_title': chunk_data.get('page_title', 'Unknown'),
                'url': chunk_data.get('url', ''),
                'domain': chunk_data.get('domain', ''),
                'type': chunk_data.get('type', 'unknown'),
                'level': chunk_data.get('level', 3),
                'word_count': chunk_data.get('word_count', 0),
                'chunk_id': chunk_data.get('chunk_id', len(self.chunks) - 1)
            }

            self.chunk_metadata.append(metadata)

        if not self.chunks:
            return False

        # Build TF-IDF vectorizer and matrix

        # Enhanced TF-IDF configuration
        # Adjust parameters based on corpus size
        num_docs = len(self.chunks)

        if num_docs < 5:
            # For very small corpora, use minimal constraints
            min_df = 1
            max_df = 1.0
            ngram_range = (1, 2)  # Limit to bigrams for small corpora
        else:
            min_df = max(1, min(2, num_docs // 4))
            max_df = min(0.95, max(0.6, (num_docs - 1) / num_docs))
            ngram_range = (1, 3)

        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words='english',
            ngram_range=ngram_range,
            sublinear_tf=True,   # Use sublinear term frequency scaling
            min_df=min_df,       # Adjust based on corpus size
            max_df=max_df        # Adjust based on corpus size
        )

        # Fit and transform
        self.tfidf_matrix = self.vectorizer.fit_transform(self.chunks)


        # Cache the processed data
        try:
            cache_data = {
                'chunks': self.chunks,
                'chunk_metadata': self.chunk_metadata,
                'vectorizer': self.vectorizer,
                'tfidf_matrix': self.tfidf_matrix
            }

            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)


        except Exception as e:
            pass

        return True

    def preprocess_query(self, query: str) -> str:
        """Generic query preprocessing"""

        enhanced_query = query.lower().strip()

        # Generic technical terms expansion
        technical_expansions = {
            # Programming terms
            'function': 'function method procedure',
            'class': 'class object type',
            'api': 'api interface endpoint',
            'database': 'database db data storage',
            'server': 'server backend service',
            'client': 'client frontend application',

            # Common concepts
            'install': 'install setup configure deployment',
            'config': 'config configuration settings',
            'tutorial': 'tutorial guide example walkthrough',
            'error': 'error exception bug issue',
            'performance': 'performance optimization speed',
            'security': 'security authentication authorization',

            # Documentation terms
            'documentation': 'documentation docs guide reference',
            'example': 'example sample code snippet',
            'how to': 'how to tutorial guide steps',
        }

        # Apply expansions
        for term, expansion in technical_expansions.items():
            if term in enhanced_query:
                enhanced_query += f" {expansion}"

        # Add domain-specific terms if we can detect the domain
        if hasattr(self, 'structured_data') and self.structured_data:
            domains = self.structured_data.get('metadata', {}).get('domains', [])
            if domains:
                # Simple domain detection for common sites
                for domain in domains:
                    if 'github' in domain:
                        enhanced_query += " code repository github"
                    elif any(doc_term in domain for doc_term in ['docs', 'documentation']):
                        enhanced_query += " documentation reference guide"
                    elif 'api' in domain:
                        enhanced_query += " api endpoint method"
                    elif any(blog_term in domain for blog_term in ['blog', 'medium', 'dev']):
                        enhanced_query += " article tutorial blog post"

        return enhanced_query

    def retrieve_context(self, query: str, top_k: int = 5) -> Tuple[List[str], List[Dict]]:
        """Retrieve relevant context with enhanced scoring"""

        if not self.chunks or not hasattr(self, 'tfidf_matrix'):
            return [], []

        # Preprocess query
        enhanced_query = self.preprocess_query(query)

        # Transform query using the fitted vectorizer
        query_vector = self.vectorizer.transform([enhanced_query])

        # Calculate similarities
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()

        # Apply boosting based on content type and structure
        boosted_scores = []
        for i, sim_score in enumerate(similarities):
            metadata = self.chunk_metadata[i]

            boost_factor = 1.0

            # Boost based on content type
            content_type = metadata.get('type', 'unknown')
            if content_type == 'code_example':
                boost_factor *= 1.3
            elif content_type == 'complete_section':
                boost_factor *= 1.2

            # Boost based on section level (higher level = more important)
            level = metadata.get('level', 3)
            if level <= 2:  # h1, h2
                boost_factor *= 1.2
            elif level == 3:  # h3
                boost_factor *= 1.1

            # Boost if query terms appear in title
            title = metadata.get('title', '').lower()
            query_words = query.lower().split()
            title_matches = sum(1 for word in query_words if word in title)
            if title_matches > 0:
                boost_factor *= 1 + (0.1 * title_matches)

            # Boost based on word count (prefer substantial content)
            word_count = metadata.get('word_count', 0)
            if word_count > 100:
                boost_factor *= 1.1
            elif word_count > 200:
                boost_factor *= 1.15

            boosted_score = sim_score * boost_factor
            boosted_scores.append(boosted_score)

        # Get top results
        top_indices = np.argsort(boosted_scores)[-top_k:][::-1]

        contexts = []
        metadata_list = []

        for idx in top_indices:
            if boosted_scores[idx] > 0.1:  # Minimum threshold
                contexts.append(self.chunks[idx])

                # Enhanced metadata
                meta = self.chunk_metadata[idx].copy()
                meta['similarity_score'] = similarities[idx]
                meta['boosted_score'] = boosted_scores[idx]
                metadata_list.append(meta)

        return contexts, metadata_list

    def demo_query(self, query: str, top_k: int = 5) -> str:
        """Perform a demonstration query with detailed output"""

        if not self.chunks:
            return "âŒ System not initialized. Please run scrape_and_process_website() or process_structured_documents() first."

        print(f"\nðŸ” Enhanced Query: {self.preprocess_query(query)}")
        print("=" * 80)

        contexts, metadata = self.retrieve_context(query, top_k)

        if not contexts:
            return "âŒ No relevant documents found for this query."

        print(f"âœ… Found {len(contexts)} relevant chunks:\n")

        result_text = f"âœ… Found {len(contexts)} relevant chunks:\n\n"

        for i, (context, meta) in enumerate(zip(contexts, metadata), 1):
            domain = meta.get('domain', 'Unknown')
            page_title = meta.get('page_title', 'Unknown Page')
            section_title = meta.get('section_title', 'No Section')
            content_type = meta.get('type', 'unknown')
            score = meta.get('boosted_score', 0)
            base_score = meta.get('similarity_score', 0)
            word_count = meta.get('word_count', 0)

            print(f"ðŸ“„ Result {i}: {page_title} - {section_title}")
            print(f"   Domain: {domain}")
            print(f"   Type: {content_type}")
            print(f"   Relevance: {score:.3f} (base: {base_score:.3f})")
            print(f"   Words: {word_count}")
            print(f"   Preview: {context[:200]}...\n")

            result_text += f"ðŸ“„ Result {i}: {page_title} - {section_title}\n"
            result_text += f"   Domain: {domain}\n"
            result_text += f"   Type: {content_type}\n"
            result_text += f"   Relevance: {score:.3f} (base: {base_score:.3f})\n"
            result_text += f"   Words: {word_count}\n"
            result_text += f"   Preview: {context[:200]}...\n\n"

        return result_text

    def rag_query(self, query: str, top_k: int = 5, model: str = "llama2",
                  ollama_url: str = "http://localhost:11434") -> str:
        """Perform RAG query with Ollama integration"""

        contexts, _ = self.retrieve_context(query, top_k)

        if not contexts:
            return "No relevant information found for your query."

        # Build context
        context_text = "\n\n".join(contexts)

        # Create prompt
        prompt = f"""Based on the following information, please answer the question accurately and comprehensively.

Context information:
{context_text}

Question: {query}

Please provide a detailed answer based on the context provided. If the context doesn't contain enough information to answer the question completely, please indicate what information is missing."""

        try:
            # Query Ollama
            response = requests.post(
                f"{ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=60
            )

            if response.status_code == 200:
                return response.json()["response"]
            else:
                return f"Error querying Ollama: {response.status_code}"

        except requests.exceptions.RequestException as e:
            return f"Error connecting to Ollama: {e}. Make sure Ollama is running with 'ollama serve'."



# Demo code moved to examples/rag_system_demo.py
